{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember the least and make the most**\n",
    "\n",
    "#### Representation: \n",
    ">attributes-variables = neurons in input-layer\n",
    "\n",
    "----\n",
    ">target-variables = neurons in output-layer\n",
    "\n",
    "----\n",
    ">Learning-algos = #nodes-each-layer + activation-f(a)-each-layer + search(optimization)\n",
    "\n",
    "----\n",
    "> training/fitting-model = training-set + initial-weights + learning-algos + enough-iterations\n",
    "\n",
    "----\n",
    "> trained-model = #nodes-each-layer + activation-f(a)-each-layer + best-weights-found\n",
    "\n",
    "----\n",
    "> purpose-of-training = finding the best weights\n",
    "\n",
    "----\n",
    "> predictions = test-set + trained-model\n",
    "\n",
    "#### Evaluation\n",
    "\n",
    "----\n",
    "> model-good-bad = Error = $(y^{(i)} - \\hat{y}^{(i)})^2$, i = data index, $()^2$ is better math\n",
    "\n",
    "----\n",
    "> good-model = $Error_{minimized}$\n",
    "\n",
    "#### Optimization\n",
    "\n",
    "----\n",
    "> weights-root-2-everything = $Error_{(w_{i,j})}$ ===> $w_{i,j}$ is **tuning knob** to achieve $Error_{minimum}$\n",
    "\n",
    "----\n",
    "> weights + Error ==> derivative ==> tuning-knob == 1-unit-weight-change **Convert** (?)-unit-direction-Error-change \n",
    "- derivative guide where and how-much weights update themselves\n",
    "- update weights towards $Error_{minimum}$ ==> weights = weights -=/+= learning_rate * $\\frac{dError}{dw}$\n",
    "\n",
    "----\n",
    "> **Derivative is constantly changing, so weights must constantly update**\n",
    "- $\\Delta{w}$ => $\\Delta{Error}$ => $\\Delta{\\frac{dE}{dw}}$=> $\\Delta$magnifying-power of tuning-knob => 1-unit-weight-change **Convert** (?+$\\Delta$)-unit-direction-Error-change \n",
    "\n",
    "----\n",
    "> forward-pass: given weights + data == updated-model + data => prediction => Error\n",
    "\n",
    "----\n",
    "> backward-pass: Error => $\\frac{dError}{dw}$ => new-weights = weights -=/+= learning_rate * $\\frac{dError}{dw}$     \n",
    "\n",
    "----    \n",
    "> forward-pass:  new-weights + data == new-model + data => new-prediction => new-Error  \n",
    "\n",
    "----\n",
    "> backward-pass: new-Error => new-$\\frac{dError}{dw}$ => $new_2$-weights = new-weights -=/+= learning_rate * new-$\\frac{dError}{dw}$\n",
    "\n",
    "----      \n",
    "> iterating forward-backward-pass is the way to reach optimal\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Equation vs Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal Equation**: a fancy math calculation\n",
    "\n",
    "Advantages: \n",
    "> no worry of learning_rate\n",
    "\n",
    "----\n",
    "> a lot of features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symbols and Notations\n",
    "> x, y in terms of matrix and vector\n",
    "\n",
    "----\n",
    "> more attributes: $(x_1, x_2, x_3, x_4, ...)$, target variable: $y$\n",
    "\n",
    "----\n",
    "> $x_j^{(i)}$ = the value of jth feature of the ith data example, $j\\in{n}, i\\in{m}$, $n$: num of features, $m$: is num of data points\n",
    "\n",
    "----\n",
    "> a row of data or a data point = a vertical vector = dimension (n,1)\n",
    "\n",
    "----\n",
    "> Multivariate linear regression model is expressed in vector-vector-multiplication\n",
    "- as $\\theta, x$ are **both vertical vector**\n",
    "- $h_{\\theta}(x) = \\theta_0x_0 + \\theta_1x_1 + ... = \\theta^Tx$, ($x_0 = 1$)\n",
    "- `np.dot(a,b)` for all vector and matrix multiplication; $a*c$ as scalar vector multiplication, $a, b \\in{vector}, c \\in{scalar}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### np.dot can do both scalar-vector and matrix-matrix operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "<class 'numpy.int64'>\n",
      "[3 6]\n",
      "[3 6]\n",
      "<class 'numpy.ndarray'>\n",
      "[3 6]\n",
      "[[3 6]]\n",
      "<class 'numpy.ndarray'>\n",
      "[[11]]\n",
      "[[3 4]\n",
      " [6 8]]\n",
      "[[11 11 11]]\n",
      "[[11 11 11]\n",
      " [11 11 11]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# scalar and vector multiplication\n",
    "print(np.dot(3,4))\n",
    "print(type(np.dot(3,4)))\n",
    "print(np.dot([1,2],3))\n",
    "print(np.dot(3, [1,2]))\n",
    "print(type(np.dot(3, [1,2])))\n",
    "print(np.dot(3, np.array([1,2])))\n",
    "# dimension makes a difference\n",
    "print(np.dot(np.array([1,2], ndmin=2),3))\n",
    "print(type(np.dot(np.array([1,2], ndmin=2),3)))\n",
    "\n",
    "# vector and matrix multiplication: (n,m) (m,n) must match\n",
    "print(np.dot(np.array([1,2], ndmin=2),np.array([3,4], ndmin=2).T))\n",
    "# be careful\n",
    "print(np.dot(np.array([1,2], ndmin=2).T,np.array([3,4], ndmin=2)))\n",
    "# most case usage\n",
    "print(np.dot(np.array([1,2], ndmin=2),np.array([[3,4],[3,4],[3,4]], ndmin=2).T))\n",
    "print(np.dot(np.array([[1,2],[1,2]], ndmin=2),np.array([[3,4],[3,4],[3,4]], ndmin=2).T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://lh3.googleusercontent.com/dKdGZ0GuPpflog5S9Iin6iZ6WvJVFW5myQdXO4oDFiYKk-KeqOw87aDpUfh15-R08sgdS6IMKoGhXRZtg4UOIjSfyaM2SwiVQ_ECzqLR3D2k4dTkMVEAs5SNQ80EL563ijSolg5gN9vYumAUoIHxTbFDzJ40RlCPXKZ3u39tJ2ogxc1AvBfT0r3G0ma_1Q3cZEoQG14Zj8KHjHHpyDFdOlbYmZai_GC4FtAlEEkmQW476NGO7H4sjM74mm00GMIcqWs-4sr9zSFo9tysRE1LtFYVmLlr4Gs0V3kJpNDhcf0mXhjDF0K_DqB-tND0poPb0oZYzKTBniusqDCRynE6KXK0oJ0LRihxOBqlb8oqQ5XIaISotLIIJMRyLIc5GGQeYpIJ4rgvcCXd_sQzxfRo4OqQxjITd5MoWzoRSOwLVCEJ8cs1FNyUcUtnhZb4_eHbFXmndHfJNU7hgpzeIG4kcycLFx-nK9Mq1TXHvrxtimiT8LOBZrH9gBNSooHNe0p6uJbz14iyC6PLmLLQoJ5z6L8b3frsdYOescZ4seYUsV814duxxl_8cjTdKlWCvXQMEKN9Lxb0aR4Qx419-RUrLfsjCuPqi6SmnBPtwaydOxUHnXy1fZY=w1416-h748-no\" width=\"300\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(width=300, height=200, url=\"https://lh3.googleusercontent.com/dKdGZ0GuPpflog5S9Iin6iZ6WvJVFW5myQdXO4oDFiYKk-KeqOw87aDpUfh15-R08sgdS6IMKoGhXRZtg4UOIjSfyaM2SwiVQ_ECzqLR3D2k4dTkMVEAs5SNQ80EL563ijSolg5gN9vYumAUoIHxTbFDzJ40RlCPXKZ3u39tJ2ogxc1AvBfT0r3G0ma_1Q3cZEoQG14Zj8KHjHHpyDFdOlbYmZai_GC4FtAlEEkmQW476NGO7H4sjM74mm00GMIcqWs-4sr9zSFo9tysRE1LtFYVmLlr4Gs0V3kJpNDhcf0mXhjDF0K_DqB-tND0poPb0oZYzKTBniusqDCRynE6KXK0oJ0LRihxOBqlb8oqQ5XIaISotLIIJMRyLIc5GGQeYpIJ4rgvcCXd_sQzxfRo4OqQxjITd5MoWzoRSOwLVCEJ8cs1FNyUcUtnhZb4_eHbFXmndHfJNU7hgpzeIG4kcycLFx-nK9Mq1TXHvrxtimiT8LOBZrH9gBNSooHNe0p6uJbz14iyC6PLmLLQoJ5z6L8b3frsdYOescZ4seYUsV814duxxl_8cjTdKlWCvXQMEKN9Lxb0aR4Qx419-RUrLfsjCuPqi6SmnBPtwaydOxUHnXy1fZY=w1416-h748-no\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://lh3.googleusercontent.com/JeJ3Q4NoZBwXLhX6fMCprF-Bbq9VA_UQhtm31sGWMqBhC5okgiRAdhpAgsmOLJLwOet50B9aDuy7dYhQNz6WvrL_AOA1Pp9Qyaq1O12oqdU183ZKEL7xqq-DEqOz_U8nfi6zc0FPGklxBktQ20_Pc8SuiRk8Z8emCcbLsiWb_L7zX41-Ax4qsJeL-Aq_EtfP-Em2EmGjZBxel-iu-xHc4K_vpbuLTt7vuK-PcLWyB-Js3NKkcYoO9kScZV8_75nbOV3OPWb6XjwbZcdEbq88w2EdCzexdMFkJgwjWoI4T_q7tA0ViTXtURB0V4D7_6xNtO9YIFhFGKj5IYQRkzVyXFr5xpfB8awRnlls6n4-9T-OVbiu07kkg533n3U0_qN_NREQNL9yTdNFKkSZxLP0P2M_qLjObe-YeKHXrPkOhP8zJPL9-yHtJ0VhH035cDAO2Q5LO-IMCtg4Rrn3YK0mTbKJwPpN_rUW8eoYqJKM4mxJa7XzVugxp6yNzOCVugNNUaaDqbqDK75I5VLMIp_4QFXR2djfEhOu5ptRBxvoC-8y81zMCOZGaVR0ispm93XA75Lv-AS_08d-F9piXajOMaiw6gkllAVVE6WY7ptejAcb7L6BUTg=w916-h840-no\" width=\"300\" height=\"200\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(width=300, height=200, url=\"https://lh3.googleusercontent.com/JeJ3Q4NoZBwXLhX6fMCprF-Bbq9VA_UQhtm31sGWMqBhC5okgiRAdhpAgsmOLJLwOet50B9aDuy7dYhQNz6WvrL_AOA1Pp9Qyaq1O12oqdU183ZKEL7xqq-DEqOz_U8nfi6zc0FPGklxBktQ20_Pc8SuiRk8Z8emCcbLsiWb_L7zX41-Ax4qsJeL-Aq_EtfP-Em2EmGjZBxel-iu-xHc4K_vpbuLTt7vuK-PcLWyB-Js3NKkcYoO9kScZV8_75nbOV3OPWb6XjwbZcdEbq88w2EdCzexdMFkJgwjWoI4T_q7tA0ViTXtURB0V4D7_6xNtO9YIFhFGKj5IYQRkzVyXFr5xpfB8awRnlls6n4-9T-OVbiu07kkg533n3U0_qN_NREQNL9yTdNFKkSZxLP0P2M_qLjObe-YeKHXrPkOhP8zJPL9-yHtJ0VhH035cDAO2Q5LO-IMCtg4Rrn3YK0mTbKJwPpN_rUW8eoYqJKM4mxJa7XzVugxp6yNzOCVugNNUaaDqbqDK75I5VLMIp_4QFXR2djfEhOu5ptRBxvoC-8y81zMCOZGaVR0ispm93XA75Lv-AS_08d-F9piXajOMaiw6gkllAVVE6WY7ptejAcb7L6BUTg=w916-h840-no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/mmclAQ-UlbE?list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&amp;ecver=1\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/mmclAQ-UlbE?list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&amp;ecver=1\" frameborder=\"0\" allowfullscreen></iframe>')\n",
    "# 4:11s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature scaling help reach optimal more efficiently\n",
    "- try constrain within $-1 \\le x \\le 1$\n",
    "- slightly beyond or below is fine\n",
    "- or mean normalization to constraint $-0.5 \\le x \\le 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/yQci-wS0iMw?list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&amp;ecver=1\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/yQci-wS0iMw?list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&amp;ecver=1\" frameborder=\"0\" allowfullscreen></iframe>')\n",
    "# 2:09s - 5:26s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make sure gradient descent is working\n",
    "- plot Error vs #iterations, Error should decrease on every iteration\n",
    "- set convergence when error less than $10^{-3}$\n",
    "- situation when Error keep increasing or up-down cycling, probably due to learning_rate too high (5:42s)\n",
    "- try different $alpha$ from 1, 0.1, 0.01, 0.001 (8:34s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/mlHcA-VCyN0?list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&amp;ecver=1\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/mlHcA-VCyN0?list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&amp;ecver=1\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose features and non-linear functions\n",
    "- select and create features and make polynomial function structure is like design neural structure for neural network model building\n",
    "- create new features from existing features which would make more sense(2:11s)\n",
    "- quadratic is not as meaningful as a cubic function for the problem (3:18s)\n",
    "- feature scaling is applicable to new-created features (5:14s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/8p3S9aR3n4o?list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&amp;ecver=1\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/8p3S9aR3n4o?list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&amp;ecver=1\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal equation\n",
    "- Not gradient descent, No iteration but get the optimal weights in one shot\n",
    "- set derivative of Error = 0, to directly calculate the change of weights\n",
    "- no need to worry learning_rate\n",
    "- no need to worry about feature scaling\n",
    "- example (4:26s) : 4 data examples, X and y, m examples, n+1 features\n",
    "- Normal equation: calculate weights minimize Error in one shot $\\theta = (X^TX)^{-1}X^Ty$\n",
    "- pros and cons comparing gradient descent and normal equation (13:30s)\n",
    "- when to use normal equation: (15:00s)\n",
    "    - as long as num of features less than 1000, Ng prefer normal equation \n",
    "    - for some specific linear regression, normal equation is faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/rnlti7rgns0?list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&amp;ecver=1\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/rnlti7rgns0?list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&amp;ecver=1\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal equaltion and non-invertibility\n",
    "- Normal equation use inversion to find best weights\n",
    "- non-invertible matrix: rarely happen, when it happends software will take care of it\n",
    "- causes of non-invertbility: \n",
    "    - redundant features (linearly dependent on each other): $x_1$ = size in feet, $x_2$=size in meters\n",
    "    - too many features \n",
    "- solutions: delete some features for redundant or regularization for too many features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/t-5Take484A?list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&amp;ecver=1\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<iframe width=\"500\" height=\"300\" src=\"https://www.youtube.com/embed/t-5Take484A?list=PLZ9qNFMHZ-A4rycgrgOYma6zxF4BZGGPW&amp;ecver=1\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
